{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2dda121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/VSCode/Freight-Prediction/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stefan/VSCode/Freight-Prediction\n",
      "\n",
      "ðŸš€ Starting full freight pipeline...\n",
      "\n",
      "ðŸ“¦ Fetching all raw data sources...\n",
      "\n",
      "ðŸ”¹ Fetching BPI...\n",
      "ðŸ”¹ Fetching Brent...\n",
      "ðŸ”¹ Fetching Commodities...\n",
      "ðŸ”¹ Fetching Port Congestion Metrics...\n",
      "ðŸ”¹ Fetching GSCPI...\n",
      "ðŸ”¹ Fetching Targets (Gulf & PNW)...\n",
      "ðŸ”¹ Fetching Trade Volume...\n",
      "\n",
      "âœ… All datasets fetched and saved to data/processed/\n",
      "âœ… All datasets merged and saved to data/processed/all_data.csv\n",
      "\n",
      "ðŸš€ Running all models for target: Gulf\n",
      "\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=568.113, Time=0.07 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=577.172, Time=0.01 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=571.522, Time=0.02 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=572.791, Time=0.02 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=575.964, Time=0.01 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=573.201, Time=0.02 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=574.346, Time=0.03 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0] intercept   : AIC=568.976, Time=0.13 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0] intercept   : AIC=569.264, Time=0.09 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=573.328, Time=0.01 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0] intercept   : AIC=574.904, Time=0.03 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0] intercept   : AIC=575.330, Time=0.03 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0] intercept   : AIC=inf, Time=0.11 sec\n",
      " ARIMA(2,1,2)(0,0,0)[0]             : AIC=566.705, Time=0.04 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0]             : AIC=571.627, Time=0.02 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0]             : AIC=572.758, Time=0.02 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0]             : AIC=567.478, Time=0.07 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0]             : AIC=567.796, Time=0.07 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0]             : AIC=571.764, Time=0.01 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0]             : AIC=573.378, Time=0.03 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0]             : AIC=573.818, Time=0.02 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0]             : AIC=inf, Time=0.09 sec\n",
      "\n",
      "Best model:  ARIMA(2,1,2)(0,0,0)[0]          \n",
      "Total fit time: 0.942 seconds\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=568.113, Time=0.10 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=577.172, Time=0.00 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=571.522, Time=0.01 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=572.791, Time=0.01 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=575.964, Time=0.00 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=573.201, Time=0.02 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=574.346, Time=0.04 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0] intercept   : AIC=568.976, Time=0.16 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0] intercept   : AIC=569.264, Time=0.09 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=573.328, Time=0.02 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0] intercept   : AIC=574.904, Time=0.03 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0] intercept   : AIC=575.330, Time=0.03 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0] intercept   : AIC=inf, Time=0.13 sec\n",
      " ARIMA(2,1,2)(0,0,0)[0]             : AIC=566.705, Time=0.05 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0]             : AIC=571.627, Time=0.02 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0]             : AIC=572.758, Time=0.02 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0]             : AIC=567.478, Time=0.06 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0]             : AIC=567.796, Time=0.08 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0]             : AIC=571.764, Time=0.01 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0]             : AIC=573.378, Time=0.02 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0]             : AIC=573.818, Time=0.02 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0]             : AIC=inf, Time=0.10 sec\n",
      "\n",
      "Best model:  ARIMA(2,1,2)(0,0,0)[0]          \n",
      "Total fit time: 1.027 seconds\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=564.451, Time=0.07 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=573.787, Time=0.01 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=568.119, Time=0.01 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=569.402, Time=0.01 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=572.578, Time=0.00 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=569.738, Time=0.02 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=570.928, Time=0.03 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.08 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0] intercept   : AIC=565.680, Time=0.10 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=569.933, Time=0.01 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0] intercept   : AIC=571.421, Time=0.05 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0] intercept   : AIC=571.840, Time=0.02 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0] intercept   : AIC=inf, Time=0.13 sec\n",
      " ARIMA(2,1,2)(0,0,0)[0]             : AIC=563.114, Time=0.04 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0]             : AIC=568.202, Time=0.02 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0]             : AIC=569.368, Time=0.02 sec\n",
      " ARIMA(3,1,2)(0,0,0)[0]             : AIC=563.990, Time=0.09 sec\n",
      " ARIMA(2,1,3)(0,0,0)[0]             : AIC=564.276, Time=0.08 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0]             : AIC=568.393, Time=0.01 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0]             : AIC=569.939, Time=0.02 sec\n",
      " ARIMA(3,1,1)(0,0,0)[0]             : AIC=570.375, Time=0.03 sec\n",
      " ARIMA(3,1,3)(0,0,0)[0]             : AIC=inf, Time=0.11 sec\n",
      "\n",
      "Best model:  ARIMA(2,1,2)(0,0,0)[0]          \n",
      "Total fit time: 0.962 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:53:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to reports/models_saved/Gulf_uni_prophet_model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:53:54 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:53:54 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'seasonality_prior_scale': 1.0, 'seasonality_mode': 'multiplicative', 'changepoint_prior_scale': 0.01}\n",
      "Model saved to reports/models_saved/Gulf_multi_prophet_lagged_model.joblib\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Model saved to reports/models_saved/Gulf_svm_model.joblib\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "XGBoost Test MAE: 0.82\n",
      "XGBoost RÂ² Score: 0.753\n",
      "Best Parameters: {'subsample': 0.7, 'reg_lambda': 1.0, 'reg_alpha': 0, 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "Model saved to reports/models_saved/Gulf_xgboost_model.joblib\n",
      "\n",
      "âœ… All models executed and results saved to reports/models/\n",
      "\n",
      "\n",
      "âœ… All stages complete!\n",
      "ðŸ“¦ Final dataset:      data/processed/processed.csv\n",
      "ðŸ“Š Model results:      reports/models/\n",
      "ðŸ“ˆ Ready for review.\n",
      "\n",
      "/Users/stefan/VSCode/Freight-Prediction\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# This line of code changes the parent directory, do not run it more than once or it will cause issues as it will change the working directory.\n",
    "%cd .. \n",
    "\n",
    "# Remove the folders 'reports/plots', 'reports/models_saved', and 'reports/models' if they exist.\n",
    "# This ensures that any previous model results, plots, or saved models are deleted before running the main function,\n",
    "# so that all outputs are freshly generated in the notebook workflow.\n",
    "for folder in ['reports/plots', 'reports/models_saved', 'reports/models']:\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "\n",
    "# Import the main function from the main module and execute it to generate all necessary files and models for the notebook workflow.\n",
    "from main import run_program\n",
    "run_program()\n",
    "\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9c5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to import the necessary libraries and modules for data analysis and visualization.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from utils.diagnostics import visualize_missing_data, compute_mutual_information\n",
    "from utils.preprocessing import before_interpolate, interpolate, compute_seasonal_features, compute_bpi_volatility\n",
    "from utils.compare_models import parse_model_results, plot_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13186ac",
   "metadata": {},
   "source": [
    "# Freight Forecasting Project Overview\n",
    "## Problem Statement\n",
    "\n",
    "Freight markets are notoriously volatile and influenced by a mix of macroeconomic, operational, and seasonal factors. This project builds a full freight forecasting pipeline based on open-source economic and maritime datasets. Predicting weekly freight prices can aid in:\n",
    "\n",
    "- Chartering and logistics planning\n",
    "- Risk analysis for shipping portfolios\n",
    "- Strategic commodity trading\n",
    "\n",
    "We incorporate relevant features such as:\n",
    "- Global commodity and shipping prices (e.g., Brent crude, BPI)\n",
    "- Port-level congestion indicators\n",
    "- Agricultural trade flows (e.g., PNW, Corn price)\n",
    "- Derived indicators like seasonal components and volatility\n",
    "\n",
    "This notebook demonstrates a comprehensive freight forecasting pipeline that collects multiple datasets in to a single shared dataset, which upon which I train a selection of models. The models evaluated include traditional statistical approaches (e.g., ARIMA/ARIMAX), regression models (Ridge, Lasso), Machine Learning (SVR, XGBoost), and time-series forecasters (Prophet). \n",
    "\n",
    "Each model is evaluated on:\n",
    "- 1-week-ahead prediction accuracy\n",
    "- MAE (Mean Absolute Error)\n",
    "- RÂ² Score\n",
    "- Visual comparison to actual values\n",
    "\n",
    "We will therefore proceed as follows: Explore the data, engineer features, train and evaluate models, compare their performance, and interpret the best models using SHAP.\n",
    "\n",
    "Models selection mathematical definitions\n",
    "\n",
    "## Table of Contents\n",
    "1. [Problem Statement](#1-problem-statement)  \n",
    "2. [Data Overview](#2-data-overview)  \n",
    "   1. [How data is merged, interpolated, resampled](#21-how-data-is-merged-interpolated-resampled)  \n",
    "   2. [Feature engineering](#22-Feature-engineering)\n",
    "   3. [Feature selection](#23-feature-selection)\n",
    "   4. [Model selection](#24-model-selection)\n",
    "   5. [Models selection mathematical definitions](#25-Models-selection-mathematical-definitions)\n",
    "3. [Model evaluation and comparison](#3-model-evaluation-and-comparison)  \n",
    "   1. [Evaluation summary](#33-evaluation-summary) \n",
    "   2. [Visual comparison](#32-visual-comparison) \n",
    "4. [Conclusion and Next Steps](#4-conclusion-and-next-steps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb021b7",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "After having reviewed several papers (some of which live in the `theory/` folder), it appeared reasonable to assume that the key inputs driving freight rates include fuel prices, the value of the cargo, supply and demand dynamics, and market frictions such as port congestion. However, since I do not have access to high-quality proprietary data (e.g. Clarkson, Thomson Reuters, or AIS), I needed to explore alternative open-source datasets that could serve a similar purpose. Below is an overview of the public data sources I identified and ingested for this project:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d47f5",
   "metadata": {},
   "source": [
    "| Variable              | Description                                   | Frequency  | Source \n",
    "|-----------------------|-----------------------------------------------|------------|-------------\n",
    "| `Gulf`                | Gulf freight price (target variable)          | Weekly     | [US Dpm of Agricultural](https://www.ams.usda.gov/services/transportation-analysis/gtr-datasets)\n",
    "| `PNW`                 | Pacific Northwest freight price               | Weekly     | [US Dpm of Agricultural](https://www.ams.usda.gov/services/transportation-analysis/gtr-datasets)\n",
    "| `bpi`                 | Baltic Panamax Index                          | Daily      | [See Capital Markets](https://seecapitalmarkets.com/IndexDetailed?Id=430572) \n",
    "| `brent_price`         | Brent crude oil price                         | Daily      | [US Energy Information](https://www.eia.gov/dnav/pet/hist/RBRTED.htm)\n",
    "| `corn_price`          | U.S. corn price                               | Daily      | [See Capital Markets](https://seecapitalmarkets.com/Commodity?Id=72063)\n",
    "| `wheat_price`         | U.S. wheat price                              | Daily      | [See Capital Markets](https://seecapitalmarkets.com/Commodity?Id=72061)\n",
    "| `gscpi`               | Global Supply Chain Pressure Index            | Monthly    | [New York Federal Reserve](https://www.newyorkfed.org/research/policy/gscpi#/interactive)\n",
    "| `trade_vol`           | Trade volume index                            | Monthly    | [CPB World Trade Monitor (EU Com)](https://www.cpb.nl/en/world-trade-monitor-december-2024) \n",
    "| `ships_anchored`      | Number of ships anchored (weekly avg)         | Weekly     | [U.S. Department of Transportation](https://www.bts.gov/freight-indicators#inside)\n",
    "| `ships_waiting`       | Number of ships waiting (weekly avg)          | Weekly     | [U.S. Department of Transportation](https://www.bts.gov/freight-indicators#inside)\n",
    "| `ship_cap`            | Container ship capacity                       | Weekly     | [U.S. Department of Transportation](https://www.bts.gov/freight-indicators#inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1571567",
   "metadata": {},
   "source": [
    "### 2.1 How data is merged, interpolated, resampled\n",
    "\n",
    "Because the inputs have a mixed reporting frequency (daily, weekly, and monthly) I first standardized everything to a Monday-weekly frequency. For monthly series, I forward-filled each monthâ€™s value across all weeks; for daily series, I aggregated by weekly mean. Furthermore, we can observe the dataset are of different lengths and that the target series still had gaps, so I used linear interpolation to impute missing weeks. Finally, the limited coverage of the port-congestion proxies forced me to drop any weeks without full exogenous data, leaving a total of 165 complete observations which is approximately 3,2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcc0407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b06438e0204c5bb5b7f7a067524855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output())), HBox(children=(Output(), Output()))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out1, out2, out3, out4 = widgets.Output(), widgets.Output(), widgets.Output(), widgets.Output()\n",
    "\n",
    "df_raw = pd.read_csv('data/processed/all_data.csv')\n",
    "df_processed = pd.read_csv('data/processed/processed.csv')\n",
    "\n",
    "with out1:\n",
    "    visualize_missing_data('data/processed/all_data.csv')\n",
    "\n",
    "with out2:\n",
    "    visualize_missing_data('data/processed/processed.csv')\n",
    "\n",
    "with out3:\n",
    "    before_interpolate(showplot=True)\n",
    "\n",
    "with out4:\n",
    "    interpolate(showplot=True)\n",
    "\n",
    "row1 = widgets.HBox([out1, out2])\n",
    "row2 = widgets.HBox([out3, out4])\n",
    "\n",
    "grid = widgets.VBox([row1, row2])\n",
    "display(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d276a8",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering\n",
    "\n",
    "The processed dataset adds a few derived columns:\n",
    "\n",
    "- **Trend & Seasonal Components**  \n",
    "  \n",
    "  We apply an additive seasonal decomposition to the BPI and Brent series:  \n",
    "  $$\n",
    "    y_t = T_t + S_t + R_t\n",
    "  $$  \n",
    "  - $y_t$: observed value at time $t$  \n",
    "  - $T_t$: **trend** (long-term progression)  \n",
    "  - $S_t$: **seasonal** (regular calendar-related cycles)  \n",
    "  - $R_t$: **residual** (irregular noise)  \n",
    "\n",
    "  By separating $T_t$ and $S_t$, the models can focus on structural movements rather than short-term noise.\n",
    "\n",
    "- **BPI Volatility**  \n",
    "  \n",
    "  Calculated as the 4-week rolling standard deviation of the Baltic Panamax Index.  \n",
    "  This captures sudden shocks and serves as a proxy for market uncertainty.\n",
    "\n",
    "These features (trend, seasonality, and volatility) provide cleaner signals and this can sometimes help improve forecast accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bb7542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd8f93f33f741d7ae7070cce3a80f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output())),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out1, out2 = widgets.Output(), widgets.Output()\n",
    "\n",
    "with out1:\n",
    "    compute_seasonal_features(filepath='data/processed/brent.csv', column='brent_price', showplot=True)\n",
    "\n",
    "with out2:\n",
    "    compute_seasonal_features(filepath='data/processed/bpi.csv', column='bpi', showplot=True)\n",
    "\n",
    "row1 = widgets.HBox([out1, out2])\n",
    "\n",
    "grid = widgets.VBox([row1])\n",
    "display(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9e77a",
   "metadata": {},
   "source": [
    "### Brief Analysis of Time Series Decompositions\n",
    "\n",
    "(Right) The Baltic Panamax Index decomposition highlights clear responses to major global shocksâ€”most notably the 2008 financial crisis, the COVID-19 downturn in 2020, and the onset of the Ukraine war in early 2022. The trend component captures the overall rise and fall of freight rates, while the seasonal cycle reveals recurring seasonal patterns in cargo flows.\n",
    "\n",
    "(Left) In the Brent spot price decomposition, we trace a much longer history of volatility: from the aftermath of 9/11 and the Iraq War, through the 2008 crisis and the U.S. shale boom, to the pandemic and the Ukraine conflict. While many short-term spikes are driven by OPEC decisions and geopolitical tensions, the trend and seasonal components help isolate those effects from underlying market dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### Brief Analysis of Volatility\n",
    "\n",
    "The 4-week rolling standard deviation of the BPI clearly spikes around the same major eventsâ€”2008, 2020, and 2022â€”underscoring periods of elevated market uncertainty. These volatility peaks serve as useful proxies for stress in the shipping markets, which our models could be able to leverage to improve forecast robustness.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96015800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db957b8c19645ecb575a6cf31ab6296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(), Output(), Output())),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out1, out2, out3 = widgets.Output(), widgets.Output(), widgets.Output()\n",
    "\n",
    "with out1:\n",
    "    compute_bpi_volatility(showplots=True)\n",
    "with out2:\n",
    "    df_raw.info()\n",
    "with out3:\n",
    "    df_processed.info()\n",
    "row1 = widgets.HBox([out1, out2, out3])\n",
    "#row2 = widgets.HBox([out2, out3])\n",
    "\n",
    "grid = widgets.VBox([row1])\n",
    "display(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9b63f",
   "metadata": {},
   "source": [
    "### 2.3 Feature selection\n",
    "\n",
    "Given the relatively small sample size and the large number of predictors and furthermore many of which are highly correlated (e.g., raw BPI vs. BPI trend, seasonal components of different series). There is a substantial risk that if I use all available inputs that I will destabilize coefficient estimates, obscure true feature importance, and degrade predictive performance due to overfitting.\n",
    "\n",
    "To manage this, I apply a simple two-step workflow:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Embedded Regularization: Lasso Regression**\n",
    "\n",
    "As a first step, I apply Lasso regression to the full feature set. A Lasso regression optimizes against forecasting loss (MAE) and automatically shrinks or zeroes out the weights of features that do not help predictive accuracy. This effectively performs feature selection based on out-of-sample error minimization.  \n",
    "    \n",
    "\n",
    "**Lasso Objective Function:**\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n",
    "$$\n",
    "\n",
    "All predictors with non-zero weights are retained.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Redundancy Pruning via Mutual Information (MI)**\n",
    "\n",
    "To further reduce redundant or weakly informative variables, I compute the mutual information (MI) between each feature and the target variable. MI is model-agnostic and captures both linear and nonlinear dependencies.    \n",
    "\n",
    "Features with an MI score below **0.25** are dropped from the final input set.\n",
    "\n",
    "**Mutual Information Formula:**\n",
    "\n",
    "$$\n",
    "I(X; Y) = \\iint p(x, y) \\log \\left( \\frac{p(x, y)}{p(x)\\,p(y)} \\right) dx\\,dy\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ p(x, y) $ is the joint probability density of $ X $ and $ Y $  \n",
    "- $ p(x) $, $ p(y) $ are the marginal densities  \n",
    "- The log is typically the natural logarithm (nats)\n",
    "\n",
    "**Key Properties:**\n",
    "- $ I(X; Y) = 0 $ â‡” independence  \n",
    "- $ I(X; Y) \\geq 0 $ (always non-negative)  \n",
    "- Captures nonlinear, non-monotonic relationships  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Alternatives: VIF, Correlation and AutoCorrelations**\n",
    "\n",
    "Other available diagnostics which I have included:\n",
    "- **Variance Inflation Factor (VIF):** Quantifies how much a feature's variance is inflated due to multicollinearity.\n",
    "- **Pearson Correlation Matrix:** Useful for identifying linear relationships (e.g., correlations above 0.8).\n",
    "- **Auto Correlations:** Highlights persistence or patterns over time within a single variable, which is particularly relevant for time series modeling.\n",
    "\n",
    "(Personal argument) While VIF and correlation can be helpful, this two-step strategy (Lasso + MI), provides a robust and interpretable method for building a stable feature set that balances model performance and generalizability across a broader set of models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd7383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c92ea834664f1f8a2dd14863bdcffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()), layout=Layout(align_items='center', justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/processed.csv')\n",
    "top_8 = ['PNW', 'bpi', 'brent_price', 'corn_price', 'brent_price_seasonal', 'ships_anchored', 'bpi_seasonal', 'bpi_volatility']\n",
    "\n",
    "out1, out2 = widgets.Output(), widgets.Output() \n",
    "\n",
    "with out1:\n",
    "    img = mpimg.imread('reports/plots/Lasso_Coefficients_Gulf_nonlagged.png')\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Lasso Coefficients for Target: Gulf')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "with out2:\n",
    "    compute_mutual_information(df=df, target='Gulf', feature_cols=top_8, plot=True)\n",
    "\n",
    "row = widgets.HBox([out1, out2], layout=widgets.Layout(justify_content='center', align_items='center'))\n",
    "display(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa4d67",
   "metadata": {},
   "source": [
    "### 2.4 Model Selection\n",
    "\n",
    "Given that PNW, corn_price, bpi, brent_price, and ships_anchored appear to be the most informative predictors of Gulf freight prices, and that we have a weekly dataset spanning from 2021-08-02 to 2024-09-23 (165 observations), we are working with approximately 33 observations per regressor. This constrains our choice of models, especially those requiring large sample sizes or sensitive to overfitting.\n",
    "\n",
    "Below is a non-exhaustive summary of candidate models and heuristic guidelines for their use:\n",
    "\n",
    "| Model                         | Type                      | Handles Nonlinearity | Multicollinearity Tolerance | Min. Observations per Feature | Notes |\n",
    "|------------------------------|---------------------------|----------------------|-----------------------------|-------------------------------|-------|\n",
    "| **Linear Regression**        | Linear, Parametric        | No                   | Low                         | 10â€“15                         | Requires independent predictors |\n",
    "| **Lasso Regression**         | Linear, Regularized       | No                   | Medium (auto-prunes)        | 5â€“10                          | Performs variable selection |\n",
    "| **Ridge Regression**         | Linear, Regularized       | No                   | High                        | 5â€“10                          | Retains all predictors with shrinkage |\n",
    "| **Elastic Net**              | Linear, Regularized       | No                   | Medium                      | 5â€“10                          | Hybrid of Lasso & Ridge |\n",
    "| **Decision Tree**            | Nonlinear, Non-parametric | Yes                  | High                        | 5â€“10                          | Tunable depth; risk of overfitting |\n",
    "| **Random Forest**            | Nonlinear, Ensemble       | Yes                  | High                        | 5â€“10                          | Handles many features well |\n",
    "| **XGBoost**                  | Nonlinear, Ensemble       | Yes                  | High                        | 10â€“20                         | Strong performance with tuning |\n",
    "| **Support Vector Regression**| Linear / RBF Kernel       | Yes                  | Medium                      | 10â€“15                         | Sensitive to feature scaling |\n",
    "| **k-NN Regression**          | Non-parametric            | Yes                  | High                        | 20+                           | Suffers in high dimensions |\n",
    "| **ARIMA**                    | Time Series, Univariate   | No                   | N/A                         | 50â€“100 (total)               | No exogenous variables |\n",
    "| **ARIMAX**                   | Time Series, Exogenous    | No                   | Low                         | 20â€“30 per regressor           | ARIMA with external features |\n",
    "| **SARIMAX**                  | Seasonal Time Series      | No                   | Low                         | 20â€“30 per regressor           | Adds seasonality and exogenous features |\n",
    "| **Prophet (Univariate)**     | Additive Model            | Partial (seasonality)| High                        | 30â€“50 (total)                | Good for trend + seasonality |\n",
    "| **Prophet (Multivariate)**   | Additive + Regressors     | Partial              | Medium                      | 10â€“20 per regressor           | Regressors added manually |\n",
    "| **Neural Network (MLP)**     | Deep Learning             | Yes                  | Medium                      | 50â€“100 per regressor          | Risk of overfitting with small data |\n",
    "| **LSTM / RNN**               | Recurrent Neural Net      | Yes                  | Medium                      | 100+ per regressor            | Best with long sequences |\n",
    "\n",
    "---\n",
    "\n",
    "### Selected Models\n",
    "\n",
    "For this project, I have selected a diverse set of models that reflect both classical econometric and machine learning approaches:\n",
    "\n",
    "- **Lasso Regression**\n",
    "- **Ridge Regression**\n",
    "- **XGBoost**\n",
    "- **Support Vector Regression**\n",
    "- **ARIMA**\n",
    "- **SARIMAX**\n",
    "- **Prophet (Univariate)**\n",
    "- **Prophet (Multivariate)**\n",
    "\n",
    "> Note: Although SARIMAX is implemented, it is used with non-seasonal components and multiple exogenous regressorsâ€”making it effectively an ARIMAX model in structure, albeit estimated via the SARIMAX engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc384dfa",
   "metadata": {},
   "source": [
    "### 2.4 Model Selection: Mathematical Definitions\n",
    "\n",
    "All models in this project are structured to predict one week aheadâ€”that is, the target variable is shifted forward by one step to simulate forecasting $ y_{t+1} $.\n",
    "\n",
    "---\n",
    "\n",
    "#### **ARIMAX (Lagged Exogenous Variables)**\n",
    "\n",
    "A classical time series model extending ARIMA to incorporate external regressors. In the lagged version, we use past values of predictors $ x_{t-1}, x_{t-2}, \\dots $ to forecast future values of $ y $.\n",
    "\n",
    "$$\n",
    "y_t = c + \\phi_1 y_{t-1} + \\dots + \\theta_1 \\varepsilon_{t-1} + \\dots + \\beta_1 x_{t-1}^{(1)} + \\dots + \\beta_k x_{t-1}^{(k)} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_t $: target variable (e.g., Gulf freight rate)\n",
    "- $ x_{t-1}^{(k)} $: lagged exogenous variables\n",
    "- $ \\phi $: autoregressive coefficients\n",
    "- $ \\theta $: moving average coefficients\n",
    "- $ \\beta $: exogenous coefficients\n",
    "- $ \\varepsilon_t $: white noise\n",
    "\n",
    "I also implement a non-lagged version using contemporaneous $ x_t $.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Prophet (Univariate & Multivariate)**\n",
    "\n",
    "A decomposable model developed by Facebook that forecasts time series using trend and seasonality:\n",
    "\n",
    "$$\n",
    "y(t+1) = g(t) + s(t) + h(t) + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ g(t) $: trend component  \n",
    "- $ s(t) $: seasonal component  \n",
    "- $ h(t) $: holiday effects (not used)  \n",
    "- $ \\varepsilon_t $: error term  \n",
    "\n",
    "In the multivariate case, external regressors are added linearly:\n",
    "\n",
    "$$\n",
    "y(t+1) = g(t) + s(t) + \\sum_{i} \\beta_i x_t^{(i)} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Lasso & Ridge Regression**\n",
    "\n",
    "Both models use linear regression with regularization and forecast one step ahead. Ridge penalizes the size of coefficients; Lasso can shrink some to exactly zero (embedded feature selection).\n",
    "\n",
    "- **Ridge Regression (L2 penalty):**\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n",
    "$$\n",
    "\n",
    "- **Lasso Regression (L1 penalty):**\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\sum_{i=1}^n (y_i - X_i \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Support Vector Regression (SVR)**\n",
    "\n",
    "I apply SVR with a linear kernel to predict $ y_{t+1} $. The SVR algorithm fits a function within an $ \\varepsilon $-insensitive tube while minimizing complexity.\n",
    "\n",
    "Objective:\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i - \\mathbf{w}^\\top x_i - b &\\leq \\varepsilon + \\xi_i \\\\\n",
    "\\mathbf{w}^\\top x_i + b - y_i &\\leq \\varepsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* &\\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "XGBoost builds an ensemble of trees sequentially to reduce residuals from earlier models. The final prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{k=1}^K f_k(x_i), \\quad f_k \\in \\mathcal{F}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ f_k $: individual regression tree\n",
    "- $ \\mathcal{F} $: function space of all trees\n",
    "\n",
    "The model minimizes:\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i} \\ell(y_i, \\hat{y}_i) + \\sum_{k} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "with regularization term $ \\Omega(f_k) $ to control complexity and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49064788",
   "metadata": {},
   "source": [
    "## 3. Model evaluation and comparison\n",
    "\n",
    "In this section we will evaluate the performance of several predictive models using a consistent framework. The primary goal is to assess how well each model forecasts one-week-ahead Gulf freight rates based on the same set of variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### Models are compared using two key metrics:\n",
    "\n",
    "Mean Absolute Error (MAE) â€“ average magnitude of prediction errors\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "R-squared (RÂ²) â€“ proportion of variance in the target explained by the model\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "Where:\n",
    "- $ y_{i} $: actual value\n",
    "- $ \\hat{y}_{i} $: predicted value\n",
    "- $ \\bar{y}_{i} $: means of the actual value\n",
    "- $ n $: number of observations  \n",
    "- $ R^{2} $: ranges from $-\\infty$ to 1, with 1 being a perfect fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea3bb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d02254349ff451995a95bf260174b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()), layout=Layout(align_items='center', justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = parse_model_results()\n",
    "without_prop_uni = evaluation[evaluation.Model != 'Uni Prophet Regression']\n",
    "\n",
    "out1, out2 = widgets.Output(), widgets.Output()\n",
    "with out1:\n",
    "    display(evaluation)\n",
    "with out2:\n",
    "    display(without_prop_uni)\n",
    "row = widgets.HBox([out1, out2], layout=widgets.Layout(justify_content='center', align_items='center'))\n",
    "display(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c1db5",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation Summary\n",
    "\n",
    "The best-performing model was the non-lagged specification using contemporaneous ARIMAX, which achieved the lowest MAE of $0.50$ and the highest $ R^2 = 0.903 $, indicating excellent predictive accuracy and strong alignment with observed freight rate dynamics.\n",
    "\n",
    "However, it is important to emphasize that the contemporaneous ARIMAX model is not forward-looking in the strict sense. It predicts $ y_t $ using $ x_t $, which assumes that all exogenous inputs are available at the same time as the target which is an assumption that may not hold in real-time forecasting applications.\n",
    "\n",
    "In contrast, models such as Multi-Prophet, SVR, XGBoost, and the lagged ARIMAX variant are explicitly structured to forecast $ y_{t+1} $ using information available at time $ t $. When comparing these forward-looking models on equal terms, Multi-Prophet delivered the strongest results with $ \\text{MAE} = 0.73 $ and $ R^2 = 0.761 $, significantly outperforming its univariate counterpart, which performed poorly with $ \\text{MAE} = 7.07 $ and $ R^2 = -18.267 $. As a result, the univariate Prophet model is excluded from further consideration.\n",
    "\n",
    "The Support Vector Regression (SVR) model also performed well, yielding $ \\text{MAE} = 0.77 $ and $ R^2 = 0.735 $, followed by XGBoost, which provided consistent results at $ \\text{MAE} = 0.92 $ and $ R^2 = 0.692 $, outperforming its untuned baseline. These performances place the lagged ARIMAX model in third position among forward-looking approaches, with a still respectable $ \\text{MAE} = 0.84 $ and $ R^2 = 0.710 $.\n",
    "\n",
    "Regularized linear models delivered more modest results. Lasso regression achieved $ \\text{MAE} = 1.23 $ and $ R^2 = 0.407 $, while Ridge regression followed with $ \\text{MAE} = 1.36 $ and $ R^2 = 0.348 $. However, further feature tuning led to notable improvements in Lassoâ€™s predictive performance, suggesting its usefulness in more constrained or interpretable modeling settings.\n",
    "\n",
    "This comparison underscores a key modeling distinction: although the contemporaneous ARIMAX offers excellent in-sample accuracy, its predictive utility under real-time constraints is limited unless future values of the regressors are known or separately forecasted. Accordingly, ARIMAX is best suited for explanatory modeling and nowcasting, rather than genuine out-of-sample forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6f483b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3579050072147d3aece6154fe332e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()), layout=Layout(align_items='center', justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_comparison(without_prop_uni)\n",
    "\n",
    "out1, out2 = widgets.Output(), widgets.Output() \n",
    "\n",
    "with out1:\n",
    "    img = mpimg.imread('reports/plots/comparison_r2.png')\n",
    "    fig, ax = plt.subplots(figsize=(6, 5)) \n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Model Comparison (R-squared)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "with out2:\n",
    "    img = mpimg.imread('reports/plots/comparison_mae.png')\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Model Comparison (MAE)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "row = widgets.HBox([out1, out2], layout=widgets.Layout(justify_content='center', align_items='center'))\n",
    "display(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcafba9",
   "metadata": {},
   "source": [
    "### 3.2 Visual Comparison\n",
    "Here we can see a visualize the of prediction plots for the non-lagged ArimaX and Prophet models for the Gulf dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91abcad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eabb4abf4b04b8e8b173ab767bead46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()), layout=Layout(align_items='center', justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out1, out2 = widgets.Output(), widgets.Output() \n",
    "\n",
    "with out1:\n",
    "    img = mpimg.imread('reports/plots/Gulf_arimax_prediction_plot.png')\n",
    "    fig, ax = plt.subplots(figsize=(7, 6)) \n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('ArimaX Prediction Plot for Gulf')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "with out2:\n",
    "    img = mpimg.imread('reports/plots/Gulf_multi_prophet_lagged_prediction_plot.png')\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Prophet Prediction Plot for Gulf')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "row = widgets.HBox([out1, out2], layout=widgets.Layout(justify_content='center', align_items='center'))\n",
    "display(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737e6b5",
   "metadata": {},
   "source": [
    "### 4. Conclusion and Next Steps\n",
    "\n",
    "This project aimed to benchmark a diverse set of time series and machine learning models for short-term freight rate forecasting, focusing on the Gulf route. By integrating autoregressive structures with exogenous signalsâ€”including oil prices, port congestion, and commodity trendsâ€”we systematically evaluated each model's out-of-sample predictive performance.\n",
    "\n",
    "The findings demonstrate that model architecture and data alignment are decisive factors in predictive success. The contemporaneous ARIMAX model achieved the best in-sample fit, with the lowest MAE and highest $R^2$. However, because it relies on real-time availability of exogenous inputs, it is best suited for nowcasting and explanatory analysis, rather than forward-looking prediction.\n",
    "\n",
    "Among the models explicitly structured to forecast $y_{t+1}$ using only data available at time $t$, the Multi-Prophet, SVR, and XGBoost models delivered the strongest results. These models successfully captured non-linearities and external dynamics, outperforming both traditional baselines and the lagged ARIMAX variant in terms of predictive accuracy and generalization.\n",
    "\n",
    "This analysis yields several key takeaways:\n",
    "\n",
    "- Temporal alignment is critical: \n",
    "    Forecasting future outcomes requires careful handling of target shifting and input lagging. Not all models address this implicitly.\n",
    "- Interpretability vs. flexibility: \n",
    "    ARIMAX offers a transparent, theory-driven approach; Prophet is intuitive and modular; SVR and XGBoost provide more flexible, data-driven predictions.\n",
    "- Model choice is context-dependent: \n",
    "    Practical deployment must consider trade-offs between latency, interpretability, input availability, and robustness.\n",
    "\n",
    "Next steps may include:\n",
    "\n",
    "- Extending the framework to multi-step forecasting horizons  \n",
    "- Simulating feature uncertainty and real-time data lag  \n",
    "- Developing ensemble methods to improve stability and accuracy  \n",
    "- Using SHAP or similar methods for cross-model interpretability  \n",
    "- Building scenario simulation tools to explore the impact of hypothetical policy, market, or supply chain shocks\n",
    "\n",
    "With a strong modeling foundation now established, this system is well-positioned for integration into a real-time decision support dashboard, enabling proactive freight monitoring and strategic planning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
